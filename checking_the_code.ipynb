{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "understood-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from functools import reduce\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informational-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Dataset70_30' # OR \"Dataset70_30\"( to run second time with 'Dataset80_20')\n",
    "train_path = os.path.join(filename, 'train')\n",
    "classes_path= os.path.join(filename, 'classes.txt')\n",
    "class_num=[]\n",
    "with open(classes_path, \"r\") as read_file:\n",
    "    class_num = json.load(read_file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incident-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset70_30\\classes.txt\n",
      "\n",
      "\n",
      "0\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "trainvideosarray= glob.glob(os.path.join(train_path, '*', '*.mp4'))\n",
    "print(classes_path)\n",
    "print('\\n')\n",
    "print(class_num['American Football'])\n",
    "#print(trainvideosarray)\n",
    "print(len(trainvideosarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defensive-center",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before video path\n",
      "\n",
      "Dataset70_30\\train\\American Football\\0.mp4\n",
      "after video path\n",
      "\n",
      "before tensor zeros\n",
      "\n",
      "after tensor zeros\n",
      "\n",
      "before video capture\n",
      "\n",
      "before release\n",
      "\n",
      "after rel\n",
      "\n",
      "after video capture\n",
      "\n",
      "(720, 1280, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nvideo_frames = []\\ncap = cv2.VideoCapture(video_path)\\nwhile (cap.isOpened()):\\n    ret, frame = cap.read()\\n    if ret == True:\\n        video_frames.append(frame)\\n    else:\\n        print(\"frame didnt extracted well or finished if shows uup try to delete this printing\\n\")\\n        break\\ncap.release()\\n\\ncv2.destroyAllWindows()\\n\\nprint(video_frames)\\n\\n\\n# extracting keeping\\n\\n#keeping the indexes of frames we want to take\\njumping_frames = len(video_frames)/ self.seq_size # need to take frame after this number of times\\nframe_index_array=[]\\nfor i in range(0, len(video_frames)): # data size is the video size, check if start from 0 or 1 and end with size or size+1\\n    #need to add index of frame  that devide with out reminder in self.seq_size from the specific video\\n    if(i%jumping_frames==0):\\n        frame_index_array.append(i)\\n\\n\\n\\nfor idx, id_frm in enumerate(frame_index_array):\\n    #TODO: below reading specif frame from the video(video(id_frm)), and resize it to (3,180, 220)(using pytorch or pytorch probably)\\n    tensor[idx,3,:,:] = cv2.resize(video_frames(id_frm),(3,180,220))# check if need 3 or juct 180,220\\n\\n\\n# maybe we needd to change to hotencoder\\n# label = torch.zeros(1, len(self.class_num.keys()), dtype=torch.long)\\n# label[0, self.class_num[os.path.dirname(video_path).split(\\'/\\')[-1]]] = 1\\n#reading jenre name and take it value- aka its label number\\nlabel = class_num[os.path.dirname(video_path).split(\\'\\\\\\')[-1]]\\n# label = self.class_num[os.path.dirname(video_path).split(\\'/\\')[-1]]\\nlabel = torch.tensor(label)\\nlabel = label.long()\\n\\ntensor, label\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_video = 0\n",
    "print(\"before video path\\n\")\n",
    "video_path = trainvideosarray[first_video]\n",
    "print(video_path)\n",
    "print(\"after video path\\n\")\n",
    "# TODO: below reading video\n",
    "# with open(video_path, \"r\") as read_file: # need to change to reading a video, probably using opev cv videoCapture\n",
    "#    data = json.load(read_file)\n",
    "\n",
    "#tensor = torch.zeros((len(data), 1, self.hand_points))\n",
    "print(\"before tensor zeros\\n\")\n",
    "tensor = torch.zeros(60, 3, 180, 220)\n",
    "print(\"after tensor zeros\\n\")\n",
    "\n",
    "video_frames = []\n",
    "print(\"before video capture\\n\")\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        video_frames.append(frame)\n",
    "        break\n",
    "    else:\n",
    "        print(\"frame didnt extracted well or finished if shows uup try to delete this printing\\n\")\n",
    "        break\n",
    "print(\"before release\\n\")        \n",
    "cap.release()\n",
    "print(\"after rel\\n\")\n",
    "cv2.destroyAllWindows()\n",
    "print(\"after video capture\\n\")\n",
    "print(video_frames[0].shape)\n",
    "#for id, frm in enumerate(data):\n",
    "#    tensor[id,3,:, :] = torch.Tensor(frm)\n",
    "\n",
    "# extracting the video frames\n",
    "\"\"\"\n",
    "video_frames = []\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        video_frames.append(frame)\n",
    "    else:\n",
    "        print(\"frame didnt extracted well or finished if shows uup try to delete this printing\\n\")\n",
    "        break\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(video_frames)\n",
    "\n",
    "\n",
    "# extracting keeping\n",
    "\n",
    "#keeping the indexes of frames we want to take\n",
    "jumping_frames = len(video_frames)/ self.seq_size # need to take frame after this number of times\n",
    "frame_index_array=[]\n",
    "for i in range(0, len(video_frames)): # data size is the video size, check if start from 0 or 1 and end with size or size+1\n",
    "    #need to add index of frame  that devide with out reminder in self.seq_size from the specific video\n",
    "    if(i%jumping_frames==0):\n",
    "        frame_index_array.append(i)\n",
    "\n",
    "\n",
    "\n",
    "for idx, id_frm in enumerate(frame_index_array):\n",
    "    #TODO: below reading specif frame from the video(video(id_frm)), and resize it to (3,180, 220)(using pytorch or pytorch probably)\n",
    "    tensor[idx,3,:,:] = cv2.resize(video_frames(id_frm),(3,180,220))# check if need 3 or juct 180,220\n",
    "\n",
    "\n",
    "# maybe we needd to change to hotencoder\n",
    "# label = torch.zeros(1, len(self.class_num.keys()), dtype=torch.long)\n",
    "# label[0, self.class_num[os.path.dirname(video_path).split('/')[-1]]] = 1\n",
    "#reading jenre name and take it value- aka its label number\n",
    "label = class_num[os.path.dirname(video_path).split('\\\\')[-1]]\n",
    "# label = self.class_num[os.path.dirname(video_path).split('/')[-1]]\n",
    "label = torch.tensor(label)\n",
    "label = label.long()\n",
    "\n",
    "tensor, label\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-diamond",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
